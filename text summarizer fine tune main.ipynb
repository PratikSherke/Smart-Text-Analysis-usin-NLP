{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12203180,"sourceType":"datasetVersion","datasetId":7687018}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:09:47.978583Z","iopub.execute_input":"2025-06-18T14:09:47.978750Z","iopub.status.idle":"2025-06-18T14:09:47.988889Z","shell.execute_reply.started":"2025-06-18T14:09:47.978735Z","shell.execute_reply":"2025-06-18T14:09:47.988312Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\n# Avoid TensorFlow imports by setting environment variable\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow warnings\nos.environ[\"USE_TORCH\"] = \"1\"  # Force transformers to use PyTorch\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport requests\n\n# Increase timeout for Hugging Face downloads\nos.environ[\"HF_HUB_ETAG_TIMEOUT\"] = \"60\"  # Set timeout to 60 seconds\nos.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"60\"\n\n# Clear cache directory to avoid conflicts\ncache_dir = '/kaggle/working/cache'\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir, exist_ok=True)\n\n# Verify no TensorFlow interference\ntry:\n    import tensorflow as tf\n    print(\"Warning: TensorFlow is imported. This may cause conflicts.\")\nexcept ImportError:\n    print(\"No TensorFlow import detected. Proceeding with PyTorch.\")\n\n# Check internet connectivity\ntry:\n    response = requests.get(\"https://huggingface.co\", timeout=10)\n    response.raise_for_status()\n    print(\"Internet connection verified.\")\nexcept requests.RequestException as e:\n    print(f\"Warning: No internet connection ({e}). Ensure internet is enabled in Kaggle settings or use offline mode.\")\n\n# Set device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load dataset from Kaggle input directory\ndata_path = \"/kaggle/input/nnnnpp2/your_file_name.csv\"\ntry:\n    df = pd.read_csv(data_path)\n    if 'article' not in df.columns or 'highlights' not in df.columns:\n        raise ValueError(\"Dataset must contain 'article' and 'highlights' columns\")\n    print(f\"Dataset loaded successfully. Size: {len(df)} rows\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"Dataset not found at {data_path}. Please check the path.\")\n\n# Split into train and validation sets (10% for validation)\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\nprint(f\"Training set size: {len(train_df)} rows, Validation set size: {len(val_df)} rows\")\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n\n# Initialize tokenizer\ntry:\n    tokenizer = T5Tokenizer.from_pretrained('t5-small', cache_dir=cache_dir)\nexcept Exception as e:\n    raise RuntimeError(f\"Failed to load tokenizer: {e}\")\n\n# Preprocess function to tokenize inputs and targets\ndef preprocess_function(examples):\n    inputs = [\"summarize: \" + str(doc) for doc in examples['article']]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n    labels = tokenizer([str(highlight) for highlight in examples['highlights']], max_length=128, truncation=True, padding='max_length')\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\n# Tokenize datasets\ntry:\n    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=4)\n    tokenized_val = val_dataset.map(preprocess_function, batched=True, num_proc=4)\nexcept Exception as e:\n    raise RuntimeError(f\"Tokenization failed: {e}\")\n\n# Initialize model and move to GPU\ntry:\n    model = T5ForConditionalGeneration.from_pretrained('t5-small', cache_dir=cache_dir).to(device)\nexcept Exception as e:\n    raise RuntimeError(f\"Failed to load model: {e}\")\n\n# Define training arguments optimized for Kaggle GPU\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results',\n    num_train_epochs=10,\n    per_device_train_batch_size=8,  # Suitable for 6500 rows on Kaggle GPU\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='/kaggle/working/logs',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=500,\n    save_strategy='steps',\n    save_steps=1000,\n    load_best_model_at_end=True,\n    fp16=True,  # Enable mixed precision for GPU\n    report_to='none',  # Disable wandb logging\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n)\n\n# Start training\ntry:\n    trainer.train()\nexcept Exception as e:\n    raise RuntimeError(f\"Training failed: {e}\")\n\n# Save the fine-tuned model\nmodel.save_pretrained('/kaggle/working/fine_tuned_t5')\ntokenizer.save_pretrained('/kaggle/working/fine_tuned_t5')\nprint(\"Model and tokenizer saved to /kaggle/working/fine_tuned_t5\")\n\n# Example inference\ndef summarize_text(text):\n    inputs = tokenizer(\"summarize: \" + str(text), return_tensors='pt', max_length=512, truncation=True).to(device)\n    summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# Test inference with a sample article\ntry:\n    sample_article = df['article'].iloc[0]\n    print(\"Sample Article:\", str(sample_article)[:200], \"...\")\n    print(\"Summary:\", summarize_text(sample_article))\nexcept Exception as e:\n    print(f\"Inference failed: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:00:44.002416Z","iopub.execute_input":"2025-06-18T16:00:44.002680Z","iopub.status.idle":"2025-06-18T16:30:05.224057Z","shell.execute_reply.started":"2025-06-18T16:00:44.002660Z","shell.execute_reply":"2025-06-18T16:30:05.223374Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750262461.834579      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750262461.893071      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Warning: TensorFlow is imported. This may cause conflicts.\nInternet connection verified.\nUsing device: cuda\nDataset loaded successfully. Size: 6490 rows\nTraining set size: 5841 rows, Validation set size: 649 rows\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fdb44b5de4472ba9b3bf0bb2d08ec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"210a3a545a22485b9b7509984fc2fbf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7420e432c6427d8222aeb573f7e067"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5841 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa3cfb6683144c208a9b0a1778cfa032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/649 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a158a22886034ed0829e6502ef51f32e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd6b66843e542d2ac0633ee0dd72836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"025680bfd7624f66a4081e252a7027f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31efe759de1e483698b2999631532d45"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3660/3660 28:32, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.249100</td>\n      <td>1.133768</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.209600</td>\n      <td>1.118031</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.159100</td>\n      <td>1.115570</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.157800</td>\n      <td>1.110931</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.168400</td>\n      <td>1.110257</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.145400</td>\n      <td>1.109255</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.150300</td>\n      <td>1.109499</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer saved to /kaggle/working/fine_tuned_t5\nSample Article: A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at the Kamla Nehru Zoological Par ...\nSummary: Rahul Kumar, 17, climbed into a lions' enclosure at a zoo in Ahmedabad. He ran towards the lions shouting: 'Today I kill a lion or a lion kills me!' He was rescued by zoo guards before reaching the lions.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:35:23.337985Z","iopub.execute_input":"2025-06-18T16:35:23.338526Z","iopub.status.idle":"2025-06-18T16:35:23.343745Z","shell.execute_reply.started":"2025-06-18T16:35:23.338503Z","shell.execute_reply":"2025-06-18T16:35:23.342883Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<function __main__.preprocess_function(examples)>"},"metadata":{}}],"execution_count":3}]}