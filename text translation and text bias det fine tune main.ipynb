{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12204245,"sourceType":"datasetVersion","datasetId":7687790},{"sourceId":12205272,"sourceType":"datasetVersion","datasetId":7688526}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:47:03.154705Z","iopub.execute_input":"2025-06-18T11:47:03.155272Z","iopub.status.idle":"2025-06-18T11:47:03.161275Z","shell.execute_reply.started":"2025-06-18T11:47:03.155250Z","shell.execute_reply":"2025-06-18T11:47:03.160670Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(\"hi\")\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:47:08.097635Z","iopub.execute_input":"2025-06-18T11:47:08.098207Z","iopub.status.idle":"2025-06-18T11:47:37.919061Z","shell.execute_reply.started":"2025-06-18T11:47:08.098179Z","shell.execute_reply":"2025-06-18T11:47:37.918326Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"},{"name":"stderr","text":"2025-06-18 11:47:23.600301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750247243.831917      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750247243.897313      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# !pip install transformers==4.28.0 torch pandas numpy scikit-learn --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:58:44.092009Z","iopub.execute_input":"2025-06-18T08:58:44.092578Z","iopub.status.idle":"2025-06-18T08:58:44.096045Z","shell.execute_reply.started":"2025-06-18T08:58:44.092559Z","shell.execute_reply":"2025-06-18T08:58:44.095331Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nimport os\n\n# Step 1: Load and preprocess the dataset\ndef load_data(file_path):\n    # Load CSV file\n    df = pd.read_csv(file_path)\n    \n    # Ensure expected columns\n    if 'article' not in df.columns or 'Bias_Label' not in df.columns:\n        raise ValueError(\"Dataset must have 'article' and 'Bias_Label' columns\")\n    \n    # Map string labels to integers\n    label_map = {'Neutral': 0, 'Positive Bias': 1, 'Negative Bias': 2}\n    df['label'] = df['Bias_Label'].map(label_map)\n    \n    # Check for invalid labels\n    if df['label'].isna().any():\n        raise ValueError(\"Some Bias_Label values are not 'Positive', 'Negative', or 'Neutral'\")\n    \n    # Remove rows with missing articles\n    df = df.dropna(subset=['article'])\n    \n    # Print class distribution\n    print(\"Class Distribution:\")\n    print(df['Bias_Label'].value_counts())\n    \n    return df['article'].values, df['label'].values\n\n# Step 2: Create a custom dataset class for BERT\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):  # Reduced max_length for larger dataset\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        # Tokenize text\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n    \n    def __len__(self):\n        return len(self.texts)\n\n# Step 3: Main function to fine-tune BERT\ndef fine_tune_bert(file_path='/kaggle/input/data-set/your file name_bias_LABEL.csv', output_dir='/kaggle/working/bias_model', num_epochs=11, batch_size=8):\n    # *** Epochs and Batch Size Defined Here ***\n    print(f\"Using {num_epochs} epochs and batch size of {batch_size} (effective batch size={batch_size*2} with gradient accumulation)\")\n    \n    # Load data\n    texts, labels = load_data(file_path)\n    \n    # Split data into train (80%), validation (10%), and test (10%) sets\n    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n        texts, labels, test_size=0.2, random_state=42, stratify=labels\n    )\n    val_texts, test_texts, val_labels, test_labels = train_test_split(\n        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n    )\n    \n    # Print split sizes\n    print(f\"Train set: {len(train_texts)} articles (~{len(train_texts)/6500*100:.1f}%)\")\n    print(f\"Validation set: {len(val_texts)} articles (~{len(val_texts)/6500*100:.1f}%)\")\n    print(f\"Test set: {len(test_texts)} articles (~{len(test_texts)/6500*100:.1f}%)\")\n    \n    # Initialize tokenizer\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    \n    # Create datasets\n    train_dataset = NewsDataset(train_texts, train_labels, tokenizer)\n    val_dataset = NewsDataset(val_texts, val_labels, tokenizer)\n    test_dataset = NewsDataset(test_texts, test_labels, tokenizer)\n    \n    # Initialize model\n    # Note: Warning about uninitialized weights is normal; training below will initialize them\n    model = BertForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=3\n    )\n    \n    # Move model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,  # *** Epochs Set Here (3) ***\n        per_device_train_batch_size=batch_size,  # *** Batch Size Set Here (8) ***\n        per_device_eval_batch_size=batch_size,  # *** Batch Size for Evaluation (8) ***\n        gradient_accumulation_steps=2,  # Effective batch size=16, reduces memory usage\n        eval_strategy='steps',\n        eval_steps=100,\n        save_strategy='steps',\n        save_steps=300,  # Multiple of eval_steps (3 * 100), saves ~once per epoch\n        load_best_model_at_end=True,\n        metric_for_best_model='eval_loss',\n        logging_dir='/kaggle/working/logs',\n        logging_steps=50,\n        save_total_limit=2,\n        fp16=torch.cuda.is_available(),  # Mixed precision for GPU\n        report_to='none'  # Disable WandB\n    )\n    \n    # Define compute_metrics function for evaluation\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        report = classification_report(labels, predictions, output_dict=True, zero_division=0)\n        return {\n            'accuracy': report['accuracy'],\n            'precision': report['weighted avg']['precision'],\n            'recall': report['weighted avg']['recall'],\n            'f1': report['weighted avg']['f1-score']\n        }\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n    \n    # Train the model (initializes weights, resolving the warning)\n    trainer.train()\n    \n    # Evaluate on test set\n    test_results = trainer.evaluate(test_dataset)\n    print(\"Test Set Evaluation:\", test_results)\n    \n    # Save the model and tokenizer\n    os.makedirs(output_dir, exist_ok=True)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model and tokenizer saved to {output_dir}\")\n    \n    return model, tokenizer, test_results\n\n# Step 4: Prediction function for new articles\ndef predict_bias(text, model, tokenizer):\n    labels = ['Neutral', 'Positive', 'Negative']\n    inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors='pt')\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()[0]  # Move to CPU before NumPy conversion\n    label = labels[probs.argmax()]\n    return label, probs\n\n\n\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:49:05.798635Z","iopub.execute_input":"2025-06-18T11:49:05.798910Z","iopub.status.idle":"2025-06-18T11:49:05.820956Z","shell.execute_reply.started":"2025-06-18T11:49:05.798889Z","shell.execute_reply":"2025-06-18T11:49:05.820039Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model, tokenizer, test_results=fine_tune_bert()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:49:20.099540Z","iopub.execute_input":"2025-06-18T11:49:20.100021Z","iopub.status.idle":"2025-06-18T12:18:56.386979Z","shell.execute_reply.started":"2025-06-18T11:49:20.099998Z","shell.execute_reply":"2025-06-18T12:18:56.386402Z"}},"outputs":[{"name":"stdout","text":"Using 11 epochs and batch size of 8 (effective batch size=16 with gradient accumulation)\nClass Distribution:\nBias_Label\nPositive Bias    3805\nNegative Bias    2651\nNeutral            34\nName: count, dtype: int64\nTrain set: 5192 articles (~79.9%)\nValidation set: 649 articles (~10.0%)\nTest set: 649 articles (~10.0%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3263b6d5a45411fb0e0ed383713b93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb41400a7b04583838f5391d70e17a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376980c8e8434bfaae0df45cc82777fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2e41a3a5e9405fa12674d2fc54bee3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0843c002754e4facb73cb4db654596"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1782' max='1782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1782/1782 29:17, Epoch 10/11]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.440700</td>\n      <td>0.397608</td>\n      <td>0.835131</td>\n      <td>0.830746</td>\n      <td>0.835131</td>\n      <td>0.832795</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.401800</td>\n      <td>0.440303</td>\n      <td>0.824345</td>\n      <td>0.820610</td>\n      <td>0.824345</td>\n      <td>0.822472</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.285800</td>\n      <td>0.454777</td>\n      <td>0.828968</td>\n      <td>0.824706</td>\n      <td>0.828968</td>\n      <td>0.826776</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.157900</td>\n      <td>0.530244</td>\n      <td>0.818182</td>\n      <td>0.814478</td>\n      <td>0.818182</td>\n      <td>0.816325</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.157200</td>\n      <td>0.671615</td>\n      <td>0.821263</td>\n      <td>0.818268</td>\n      <td>0.821263</td>\n      <td>0.819673</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.179300</td>\n      <td>0.886762</td>\n      <td>0.819723</td>\n      <td>0.815259</td>\n      <td>0.819723</td>\n      <td>0.816485</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.081800</td>\n      <td>1.828474</td>\n      <td>0.805855</td>\n      <td>0.815636</td>\n      <td>0.805855</td>\n      <td>0.805470</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.206300</td>\n      <td>2.328912</td>\n      <td>0.822804</td>\n      <td>0.818945</td>\n      <td>0.822804</td>\n      <td>0.818993</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.119800</td>\n      <td>2.773522</td>\n      <td>0.807396</td>\n      <td>0.806445</td>\n      <td>0.807396</td>\n      <td>0.806315</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.122200</td>\n      <td>2.886800</td>\n      <td>0.810478</td>\n      <td>0.807428</td>\n      <td>0.810478</td>\n      <td>0.808886</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.095400</td>\n      <td>2.980013</td>\n      <td>0.804314</td>\n      <td>0.800385</td>\n      <td>0.804314</td>\n      <td>0.799988</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.085500</td>\n      <td>2.855387</td>\n      <td>0.813559</td>\n      <td>0.808926</td>\n      <td>0.813559</td>\n      <td>0.810711</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.075300</td>\n      <td>2.784987</td>\n      <td>0.824345</td>\n      <td>0.820253</td>\n      <td>0.824345</td>\n      <td>0.820774</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.070900</td>\n      <td>2.756711</td>\n      <td>0.827427</td>\n      <td>0.824207</td>\n      <td>0.827427</td>\n      <td>0.825425</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.012900</td>\n      <td>2.947623</td>\n      <td>0.802773</td>\n      <td>0.806034</td>\n      <td>0.802773</td>\n      <td>0.803631</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.036300</td>\n      <td>2.842702</td>\n      <td>0.818182</td>\n      <td>0.820065</td>\n      <td>0.818182</td>\n      <td>0.818912</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.023600</td>\n      <td>2.815930</td>\n      <td>0.818182</td>\n      <td>0.824179</td>\n      <td>0.818182</td>\n      <td>0.821119</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='41' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [41/41 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test Set Evaluation: {'eval_loss': 2.7148425579071045, 'eval_accuracy': 0.8289676425269645, 'eval_precision': 0.8315285658829207, 'eval_recall': 0.8289676425269645, 'eval_f1': 0.8302460644437978, 'eval_runtime': 10.8379, 'eval_samples_per_second': 59.882, 'eval_steps_per_second': 3.783, 'epoch': 10.935384615384615}\nModel and tokenizer saved to /kaggle/working/bias_model\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"text=\"we went to manali and find that it has shortage of water.\"\n\npredict_bias(text,model,tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:31:33.358535Z","iopub.execute_input":"2025-06-18T12:31:33.358817Z","iopub.status.idle":"2025-06-18T12:31:33.375734Z","shell.execute_reply.started":"2025-06-18T12:31:33.358795Z","shell.execute_reply":"2025-06-18T12:31:33.374917Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('Negative',\n array([1.0110099e-04, 9.1619659e-03, 9.9073690e-01], dtype=float32))"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# text translation model fine tuning\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import MarianTokenizer, MarianMTModel, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the dataset\ndataset_path = \"/kaggle/input/dataset-tt/shortened_hindi_engish.csv\"\ndf = pd.read_csv(dataset_path)\n# Ensure the dataset has the correct columns\nassert 'english_sentence' in df.columns and 'hindi_sentence' in df.columns, \"Dataset must have 'english_sentence' and 'hindi_sentence' columns\"\n\n# Convert pandas DataFrame to Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['english_sentence', 'hindi_sentence']])\n\n# Split dataset into train and validation sets (90% train, 10% validation)\ntrain_dataset, eval_dataset = train_test_split(dataset.to_pandas(), test_size=0.1, random_state=42)\ntrain_dataset = Dataset.from_pandas(train_dataset)\neval_dataset = Dataset.from_pandas(eval_dataset)\n\n# Load tokenizer and model\nmodel_name = \"Helsinki-NLP/opus-mt-en-hi\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name).to(device)\n\n# Preprocess function to tokenize the data\ndef preprocess_function(examples):\n    inputs = [f\"translate English to Hindi: {text}\" for text in examples[\"english_sentence\"]]\n    targets = examples[\"hindi_sentence\"]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to datasets\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\neval_dataset = eval_dataset.map(preprocess_function, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/marianmt_en_hi_finetuned\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/logs\",\n    logging_steps=100,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=1000,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    report_to=\"none\",  # Disable wandb logging\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/marianmt_en_hi_finetuned\")\ntokenizer.save_pretrained(\"/kaggle/working/marianmt_en_hi_finetuned\")\n\n# Example inference\ndef translate(text):\n    inputs = tokenizer(f\"translate English to Hindi: {text}\", return_tensors=\"pt\", padding=True).to(device)\n    translated = model.generate(**inputs)\n    return tokenizer.decode(translated[0], skip_special_tokens=True)\n\n# Test the model\nsample_text = \"Hello, how are you?\"\ntranslated_text = translate(sample_text)\nprint(f\"Input: {sample_text}\")\nprint(f\"Translated: {translated_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:33:16.977191Z","iopub.execute_input":"2025-06-18T12:33:16.977526Z","iopub.status.idle":"2025-06-18T13:37:01.200417Z","shell.execute_reply.started":"2025-06-18T12:33:16.977499Z","shell.execute_reply":"2025-06-18T13:37:01.199656Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da242dd1f0bd4d9eb5d78e1c2d4afa40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"507e8f0aa4a84fd4b7b3ccfb02d47a35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3dd4b8f3de84d0b957c0eb140dc7e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b98536b94a94cdca28d1a0d1e13d503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f6625380d240cebe9b465930d4293b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"538a0ae339bf49838c34329b4e9a1c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4deeda1cbe4e4d4381b9c4733d3dd089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c590e107474e1a9d7fe86fad75709a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d495cd86a5145abac6799b2d7cc61ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3261 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b5ff43e1ca4ec7b36cff024acafbbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7344' max='7344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7344/7344 1:03:23, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.739300</td>\n      <td>0.723182</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.678300</td>\n      <td>0.674365</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.628400</td>\n      <td>0.650147</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.556500</td>\n      <td>0.638122</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.585700</td>\n      <td>0.624839</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.531400</td>\n      <td>0.620131</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.513800</td>\n      <td>0.612858</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.490400</td>\n      <td>0.610963</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.466800</td>\n      <td>0.605601</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.459500</td>\n      <td>0.606703</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.463700</td>\n      <td>0.602385</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.431700</td>\n      <td>0.604050</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.433100</td>\n      <td>0.603870</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.422200</td>\n      <td>0.602829</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nTranslated: आप कैसे हैं?\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"translate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T13:38:28.541421Z","iopub.execute_input":"2025-06-18T13:38:28.541704Z","iopub.status.idle":"2025-06-18T13:38:28.693752Z","shell.execute_reply.started":"2025-06-18T13:38:28.541683Z","shell.execute_reply":"2025-06-18T13:38:28.693044Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'क्या आप रोज़ाना श्रेणी के व्याख्यानों में जाते हैं ?'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}